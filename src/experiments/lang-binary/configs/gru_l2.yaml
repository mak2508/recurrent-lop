# Model Configuration
model:
  model_type: "GRU"
  input_size: 0          # Vocabulary size; will be updated by the tokenizer
  embedding_dim: 64      # Embedding dimension to be used by embedding layer
  hidden_size: 64        # Hidden size to be used by sequential model
  num_layers: 1          # Number of GRU layers
  num_classes: 2         # Number of classes
  vocab_limit: 0         # Vocabulary limit (maximum number of words in the vocabulary)
  max_length: 50         # Maximum sequence length
  dropout_rate: 0.0      # Dropout rate for regularization


# Training Configuration
training:
  algo: "BP"             # Backprop
  num_epochs: 3         # Number of epochs
  batch_size: 512        # Batch size
  learning_rate: 0.01    # Learning rate
  num_tasks: 3         # Number of tasks to repeat
  num_shuffles: 150      # Number of tasks to repeat with shuffling
  weight_decay: 0.0001   # Added for l2-regularization

# Dataset Configuration
languages: [
    'spa', 'por', 'ita', 'fra', 'ron', 'deu', 'swe', 'rus', 'hin', 'ara',
    'jpn', 'kor', 'tur', 'nld', 'ell', 'pol', 'ukr', 'ces', 'heb', 'cmn',
] # List of languages

train_sentences_per_class: 2600
test_sentences_per_class: 400
sentences_per_class: 3000  # Combined train + test

# Experiment Description
exp_desc: "gru_binary_l2"  # Experiment description
