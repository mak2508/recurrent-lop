# Model Configuration
model:
  model_type: "GRU"            # Specifies the model type (GRU)
  input_size: 0                # Vocabulary size; will be updated by the tokenizer
  embedding_dim: 64            # Embedding dimension to be used by embedding layer
  hidden_size: 64              # Hidden size to be used by sequential model
  num_layers: 1                # Number of GRU layers
  num_classes: 10              # Number of languages
  vocab_limit: 0               # Vocabulary limit (maximum number of words in the vocabulary)
  max_length: 25               # Maximum sequence length
  dropout_rate: 0.0            # Dropout rate for regularization

# Training Configuration
training:
  algo: "CBP_GRU"              # Training algorithm (Backprop)
  num_epochs: 30               # Number of epochs
  batch_size: 512              # Batch size
  learning_rate: 0.01          # Learning rate
  num_tasks: 100               # Number of tasks to repeat

# Dataset Configuration
languages: ["spa", "por", "ita", "fra", "ron", "deu", "cmn", "rus", "hin", "ara"] # List of languages
train_sentences_per_class: 2600 # Number of training sentences per class
test_sentences_per_class: 400   # Number of testing sentences per class
sentences_per_class: 3000       # Combined train + test

# Experiment Description
exp_desc: "gru_reshuffle_cbp" # Experiment description