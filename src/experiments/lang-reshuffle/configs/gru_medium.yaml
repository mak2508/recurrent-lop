# Model Configuration
model:
  model_type: "GRU"
  input_size: 0          # Vocabulary size; will be updated by the tokenizer
  embedding_dim: 64      # Embedding dimension to be used by embedding layer
  hidden_size: 64        # Hidden size to be used by sequential model
  num_layers: 1          # Number of GRU layers
  num_classes: 10        # Number of languages
  vocab_limit: 0         # Vocabulary limit (maximum number of words in the vocabulary)
  max_length: 25         # Maximum sequence length
  dropout_rate: 0.0      # Dropout rate for regularization

# Training Configuration
training:
  algo: "BP"             # Backprop
  num_epochs: 30         # Number of epochs
  batch_size: 512        # Batch size
  learning_rate: 0.01    # Learning rate
  num_tasks: 150         # Number of tasks to repeat
  num_tasks: 150      # Number of tasks to repeat with shuffling

# Dataset Configuration
languages: ['spa', 'por', 'ita', 'fra', 'ron', 'deu', 'cmn', 'rus', 'hin', 'ara'] # List of languages
train_sentences_per_class: 2600
test_sentences_per_class: 400
sentences_per_class: 3000  # Combined train + test

# Experiment Description
exp_desc: "gru_language_baseline"  # Experiment description
