# Model Configuration
model:
  model_type: "GRU"
  input_size: 1000       # Vocabulary size (input dimension)
  hidden_size: 32        # Hidden size (used as embedding dimension)
  num_layers: 1          # Number of GRU layers
  num_classes: 10        # Number of languages
  vocab_limit: 1000      # Vocabulary limit (maximum number of words in the vocabulary)
  max_length: 50         # Maximum sequence length
  dropout_rate: 0.0      # Dropout rate for regularization

# Training Configuration
training:
  algo: "BP"             # Backprop
  num_epochs: 5          # Number of epochs
  batch_size: 64         # Batch size
  learning_rate: 0.01    # Learning rate
  num_tasks: 60          # Number of tasks to repeat
  num_shuffles: 60       # Number of tasks to repeat with shuffling

# Dataset Configuration
languages: ['spa', 'por', 'ita', 'fra', 'ron', 'deu', 'cmn', 'rus', 'hin', 'ara'] # List of languages
train_sentences_per_class: 900
test_sentences_per_class: 100
sentences_per_class: 1000  # Combined train + test

# Experiment Description
exp_desc: "gru_language_baseline"  # Experiment description
